{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1933c2cc-ad65-46c5-8625-149bea622c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assignment 3\n",
    "Group Members: Janel Joson (jmjoson) and Yu Fang Ma (yfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370d752b-514a-4994-b961-74544e259665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Assignment Overview: Working with Patient Records and Encounter Notes\n",
    "\n",
    "In this final assignment, we’ll focus on patient records related to COVID-19 encounters. Our task is to analyze, process, and transform the data while applying the concepts we’ve covered throughout this course. Here's a detailed breakdown of the assignment:\n",
    "\n",
    "What Are Encounter Notes?\n",
    "An encounter note is a record that captures details about a patient’s visit with a doctor. It includes both structured and semi-structured information that is crucial for understanding the context of the visit. Here’s what an encounter note typically looks like:\n",
    "\n",
    "```\n",
    "AMBULATORY ENCOUNTER NOTE\n",
    "Date of Service: March 2, 2020 15:45-16:30\n",
    "\n",
    "DEMOGRAPHICS:\n",
    "Name: Jeffrey Greenfelder\n",
    "DOB: 1/16/2005\n",
    "Gender: Male\n",
    "Address: 428 Wiza Glen Unit 91, Springfield, Massachusetts 01104\n",
    "Insurance: Guardian\n",
    "MRN: 055ae6fc-7e18-4a39-8058-64082ca6d515\n",
    "\n",
    "PERTINENT MEDICAL HISTORY:\n",
    "- Obesity \n",
    "\n",
    "Recent Visit: Well child visit (2/23/2020)\n",
    "Immunizations: Influenza vaccine (2/23/2020)\n",
    "\n",
    "Recent Baseline (2/23/2020):\n",
    "Height: 155.0 cm\n",
    "Weight: 81.2 kg\n",
    "BMI: 33.8 kg/m² (99.1th percentile)\n",
    "BP: 123/80 mmHg\n",
    "HR: 92/min\n",
    "RR: 13/min\n",
    "\n",
    "SUBJECTIVE:\n",
    "Adolescent patient presents with multiple symptoms including:\n",
    "- Cough\n",
    "- Sore throat\n",
    "- Severe fatigue\n",
    "- Muscle pain\n",
    "- Joint pain\n",
    "- Fever\n",
    "Never smoker. Symptoms began recently.\n",
    "\n",
    "OBJECTIVE:\n",
    "Vitals:\n",
    "Temperature: 39.3°C (102.7°F)\n",
    "Heart Rate: 131.1/min\n",
    "Blood Pressure: 120/73 mmHg\n",
    "Respiratory Rate: 27.6/min\n",
    "O2 Saturation: 75.8% on room air\n",
    "Weight: 81.2 kg\n",
    "\n",
    "Laboratory/Testing:\n",
    "Comprehensive Respiratory Panel:\n",
    "- Influenza A RNA: Negative\n",
    "- Influenza B RNA: Negative\n",
    "- RSV RNA: Negative\n",
    "- Parainfluenza virus 1,2,3 RNA: Negative\n",
    "- Rhinovirus RNA: Negative\n",
    "- Human metapneumovirus RNA: Negative\n",
    "- Adenovirus DNA: Negative\n",
    "- SARS-CoV-2 RNA: Positive\n",
    "\n",
    "ASSESSMENT:\n",
    "1. Suspected COVID-19 with severe symptoms\n",
    "2. Severe hypoxemia requiring immediate intervention\n",
    "3. Tachycardia (HR 131)\n",
    "4. High-grade fever\n",
    "5. Risk factors:\n",
    "   - Obesity (BMI 33.8)\n",
    "   - Adolescent age\n",
    "\n",
    "PLAN:\n",
    "1. Face mask provided for immediate oxygen support\n",
    "2. Infectious disease care plan initiated\n",
    "3. Close monitoring required due to:\n",
    "   - Severe hypoxemia\n",
    "   - Tachycardia\n",
    "   - Age and obesity risk factors\n",
    "4. Parent/patient education on:\n",
    "   - Home isolation protocols\n",
    "   - Warning signs requiring emergency care\n",
    "   - Return precautions\n",
    "5. Follow-up plan:\n",
    "   - Daily monitoring during acute phase\n",
    "   - Virtual check-ins as needed\n",
    "\n",
    "Encounter Duration: 45 minutes\n",
    "Encounter Type: Ambulatory\n",
    "Provider: ID# e2c226c2-3e1e-3d0b-b997-ce9544c10528\n",
    "Facility: 5103c940-0c08-392f-95cd-446e0cea042a\n",
    "```\n",
    "\n",
    "\n",
    "The enocuter contains\n",
    "\n",
    "* General encounter information: \n",
    "\n",
    "  * When the encounter took place: Date and time of the visit.\n",
    "  * Demographics: Patient’s age, gender, and unique medical record identifier.\n",
    "  * Encounter details: The reason for the visit, diagnosis, and any associated costs.\n",
    "\n",
    "\n",
    "* Semi-Structured Notes:\n",
    "\n",
    "These notes mirror how doctors organize their thoughts and observations during an encounter. They generally follow a SOAP format:\n",
    "\n",
    "* Subjective: The patient’s subjective description of their symptoms, feelings, and medical concerns.\n",
    "* Objective: The doctor’s objective findings, including test results, measurements, or physical examination outcomes.\n",
    "* Assessment: The doctor’s evaluation or diagnosis based on subjective and objective information.\n",
    "* Plan: The proposed treatment plan, including medications, follow-ups, or other interventions.\n",
    "\n",
    "While some encounter notes might include additional details, the majority conform to this semi-structured format, making them ideal for analysis and transformation.\n",
    "\n",
    "* Goals for the Assignment\n",
    "\n",
    "1. Transforming Encounter Notes:\n",
    "\n",
    "Using an LLM to convert semi-structured encounter notes into a JSON format that organizes the information into structured fields. The JSON will include details such as demographics, encounter specifics, and the SOAP components of the note. Subsequently, you will need to transform the JSON data into a Parquet file, which is not only suitable for analysis in Spark but also ideal for storage later.\n",
    "Here we will use the ML classificaition to assing the objective and assessment semi-structured fields into standardized, structured fields. The medical taxonomy for this task will be the one provided by the CDC, which defines standard codes for diagnoses, symptoms, procedures, and treatments. This step ensures the structured data aligns with domain-wide medical standards, making it interoperable and ready for deeper analysis.\n",
    "\n",
    "The JSON format should capture the hierachies described in the structure below. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Basic Analytics and Visualizations:\n",
    "Using Apache Spark, perform comprehensive data analysis on the encounter data and create visualizations that reveal meaningful patterns. Your analysis must include:\n",
    "- COVID-19 Case Demographics: Case breakdown by age ranges ([0-5], [6-10], [11-17], [18-30], [31-50], [51-70], [71+])\n",
    "- Cumulative case count of Covid between the earliest case observed in the dataset and last case observed\n",
    "- Symptoms for all COVID-19 patients versus patients that admitted into the intensive care unit due to COVID.\n",
    "- Rank medications by frequency of prescription\n",
    "- Analyze medication patterns across different demographic groups (e.g., top 3 per age group)\n",
    "- Identify and plot co-morbidity information from the patient records (e.g., hypertension, obesity, prediabetes, etc.) provided in the dataset. \n",
    "- An independent group analysis: You need to develop and execute THREE original analyses that provide meaningful insights about COVID-19 patterns in this dataset. For each analysis:\n",
    "  - Clearly state your analytical question/hypothesis\n",
    "  - Justify why this analysis is valuable\n",
    "  - Show your Spark code and methodology\n",
    "  - Present results with appropriate visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c222952-5ba3-4ff7-92b0-65c169efc114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "EncounterType:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Encounter:\n",
    "    -- id --\n",
    "    date\n",
    "    time\n",
    "    -- type: EncounterType -- \n",
    "    provider_id\n",
    "    facility_id\n",
    "\n",
    "Address:\n",
    "    city\n",
    "    state\n",
    "\n",
    "Demographics:\n",
    "    -- id -- \n",
    "    name\n",
    "    date_of_birth\n",
    "    age\n",
    "    gender\n",
    "    address: Address\n",
    "    insurance\n",
    "\n",
    "Condition:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Medication:\n",
    "    code\n",
    "    description\n",
    "\n",
    "Immunization:\n",
    "    code\n",
    "    description\n",
    "    date: date\n",
    "\n",
    "VitalMeasurement:\n",
    "    code\n",
    "    value: float\n",
    "    unit\n",
    "\n",
    "BloodPressure:\n",
    "    systolic: VitalMeasurement\n",
    "    diastolic: VitalMeasurement\n",
    "\n",
    "CurrentVitals:\n",
    "    temperature: VitalMeasurement\n",
    "    heart_rate: VitalMeasurement\n",
    "    blood_pressure: BloodPressure\n",
    "    respiratory_rate: VitalMeasurement\n",
    "    oxygen_saturation: VitalMeasurement\n",
    "    weight: VitalMeasurement\n",
    "\n",
    "BaselineVitals:\n",
    "    date: date\n",
    "    height: VitalMeasurement\n",
    "    weight: VitalMeasurement\n",
    "    bmi: VitalMeasurement\n",
    "    bmi_percentile: VitalMeasurement\n",
    "\n",
    "Vitals:\n",
    "    current: CurrentVitals\n",
    "    baseline: BaselineVitals\n",
    "\n",
    "RespiratoryTest:\n",
    "    code\n",
    "    result\n",
    "\n",
    "RespiratoryPanel:\n",
    "    influenza_a: RespiratoryTest\n",
    "    influenza_b: RespiratoryTest\n",
    "    rsv: RespiratoryTest\n",
    "    parainfluenza_1: RespiratoryTest\n",
    "    parainfluenza_2: RespiratoryTest\n",
    "    parainfluenza_3: RespiratoryTest\n",
    "    rhinovirus: RespiratoryTest\n",
    "    metapneumovirus: RespiratoryTest\n",
    "    adenovirus: RespiratoryTest\n",
    "\n",
    "Covid19Test:\n",
    "    code\n",
    "    description\n",
    "    result\n",
    "\n",
    "Laboratory:\n",
    "    covid19: Covid19Test\n",
    "    respiratory_panel: RespiratoryPanel\n",
    "\n",
    "Procedure:\n",
    "    code\n",
    "    description\n",
    "    date: date\n",
    "    reasonCode\n",
    "    reasonDescription\n",
    "\n",
    "CarePlan:\n",
    "    -- id -- \n",
    "    code\n",
    "    description\n",
    "    start: date\n",
    "    stop: date\n",
    "    reasonCode\n",
    "    reasonDescription\n",
    "\n",
    "PatientRecord:\n",
    "    encounter: Encounter\n",
    "    demographics: Demographics\n",
    "    conditions: List[Condition]\n",
    "    medications: List[Medication]\n",
    "    immunizations: List[Immunization]\n",
    "    vitals: Vitals\n",
    "    laboratory: Laboratory\n",
    "    procedures: List[Procedure]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7c79c1-bcba-47d0-aa0c-b166742942c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d9c086-9366-4946-a510-521c02b9d1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using provided schema\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "from datetime import date\n",
    "\n",
    "class EncounterType(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "\n",
    "class Encounter(BaseModel):\n",
    "    date: str\n",
    "    time: str\n",
    "    provider_id: str\n",
    "    facility_id: str\n",
    "\n",
    "class Address(BaseModel):\n",
    "    city: str\n",
    "    state: str\n",
    "\n",
    "class Demographics(BaseModel):\n",
    "    name: str\n",
    "    date_of_birth: str\n",
    "    age: int\n",
    "    gender: str\n",
    "    address: Address\n",
    "    insurance: str\n",
    "\n",
    "class Condition(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class Medication(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "\n",
    "class Immunization(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "    date: str\n",
    "\n",
    "class VitalMeasurement(BaseModel):\n",
    "    code: str\n",
    "    value: float\n",
    "    unit: str\n",
    "\n",
    "class BloodPressure(BaseModel):\n",
    "    systolic: Optional[VitalMeasurement]\n",
    "    diastolic: Optional[VitalMeasurement]\n",
    "\n",
    "class CurrentVitals(BaseModel):\n",
    "    temperature: Optional[VitalMeasurement] = None\n",
    "    heart_rate: Optional[VitalMeasurement] = None\n",
    "    blood_pressure: Optional[BloodPressure] = None\n",
    "    respiratory_rate: Optional[VitalMeasurement] = None\n",
    "    oxygen_saturation: Optional[VitalMeasurement] = None\n",
    "    weight: Optional[VitalMeasurement] = None\n",
    "\n",
    "class BaselineVitals(BaseModel):\n",
    "    date: str\n",
    "    height: Optional[VitalMeasurement] = None\n",
    "    weight: Optional[VitalMeasurement] = None\n",
    "    bmi: Optional[VitalMeasurement] = None\n",
    "    bmi_percentile: Optional[VitalMeasurement] = None\n",
    "\n",
    "class Vitals(BaseModel):\n",
    "    current: Optional[CurrentVitals]\n",
    "    baseline: Optional[BaselineVitals]\n",
    "\n",
    "class RespiratoryTest(BaseModel):\n",
    "    code: str\n",
    "    result: str\n",
    "\n",
    "class RespiratoryPanel(BaseModel):\n",
    "    influenza_a: Optional[RespiratoryTest] = None\n",
    "    influenza_b: Optional[RespiratoryTest] = None\n",
    "    rsv: Optional[RespiratoryTest] = None\n",
    "    parainfluenza_1: Optional[RespiratoryTest] = None\n",
    "    parainfluenza_2: Optional[RespiratoryTest] = None\n",
    "    parainfluenza_3: Optional[RespiratoryTest] = None\n",
    "    rhinovirus: Optional[RespiratoryTest] = None\n",
    "    metapneumovirus: Optional[RespiratoryTest] = None\n",
    "    adenovirus: Optional[RespiratoryTest] = None\n",
    "\n",
    "class Covid19Test(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "    result: str\n",
    "\n",
    "class Laboratory(BaseModel):\n",
    "    covid19: Optional[Covid19Test] = None\n",
    "    respiratory_panel: Optional[RespiratoryPanel] = None\n",
    "\n",
    "class Procedure(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "    date: str\n",
    "    reasonCode: str\n",
    "    reasonDescription: str\n",
    "\n",
    "class CarePlan(BaseModel):\n",
    "    code: str\n",
    "    description: str\n",
    "    start: str\n",
    "    stop: str\n",
    "    reasonCode: str\n",
    "    reasonDescription: str\n",
    "\n",
    "class PatientRecord(BaseModel):\n",
    "    demographics: Demographics\n",
    "    encounter: Optional[Encounter] = None\n",
    "    conditions: Optional[List[Condition]] = None\n",
    "    medications: Optional[List[Medication]] = None\n",
    "    immunizations: Optional[List[Immunization]] = None\n",
    "    vitals: Optional[Vitals] = None\n",
    "    laboratory: Optional[Laboratory] = None\n",
    "    procedures: Optional[List[Procedure]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced1bf83-4b03-4dd9-b24b-1d3b5b651f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Transforming Encounter Notes\n",
    "### Transform JSON data (parsed_notes.jsonl.gz) into a Parquet file to use for analysis in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f014f53-fc76-4828-899b-2e06670b245e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, \n",
    "    StringType, IntegerType, FloatType, \n",
    "    ArrayType, MapType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "  col, count, desc, explode, array_contains, \n",
    "  dense_rank, when, struct, first, max,\n",
    "  collect_list, size, array_intersect)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc54690-971e-4e3d-82a8-68f867f64aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_pydantic_to_spark_schema(model_class):\n",
    "    \"\"\"\n",
    "    Dynamically convert Pydantic model to Spark StructType\n",
    "    \n",
    "    Args:\n",
    "        model_class (Type[BaseModel]): Pydantic BaseModel class to convert\n",
    "    \n",
    "    Returns:\n",
    "        StructType: Corresponding Spark SQL schema\n",
    "    \"\"\"\n",
    "    def _get_field_type(field_type, is_optional=False):\n",
    "        \"\"\"\n",
    "        Convert Python type annotations to Spark SQL types\n",
    "        \"\"\"\n",
    "        origin = getattr(field_type, '__origin__', None)\n",
    "        \n",
    "        # Handle Optional types\n",
    "        if origin is type(None):\n",
    "            return None\n",
    "        \n",
    "        # List handling\n",
    "        if origin is list:\n",
    "            # Get the type of list elements\n",
    "            element_type = field_type.__args__[0]\n",
    "            return ArrayType(_get_field_type(element_type))\n",
    "        \n",
    "        # Optional handling\n",
    "        if origin is type(None) or (hasattr(field_type, '__origin__') and \n",
    "                                    field_type.__origin__ is type(origin)):\n",
    "            return None\n",
    "        \n",
    "        # Nested Pydantic model handling\n",
    "        if hasattr(field_type, '__bases__') and \\\n",
    "           len(field_type.__bases__) > 0 and \\\n",
    "           field_type.__bases__[0].__name__ == 'BaseModel':\n",
    "            return _convert_model_to_struct(field_type)\n",
    "        \n",
    "        # Primitive type mapping\n",
    "        type_map = {\n",
    "            str: StringType(),\n",
    "            int: IntegerType(),\n",
    "            float: FloatType(),\n",
    "            date: StringType(),  # Convert date to string in Spark\n",
    "        }\n",
    "        \n",
    "        # Direct type match\n",
    "        if field_type in type_map:\n",
    "            return type_map[field_type]\n",
    "        \n",
    "        # Default to string for unknown types\n",
    "        return StringType()\n",
    "\n",
    "    def _convert_model_to_struct(model):\n",
    "        \"\"\"\n",
    "        Convert a Pydantic model to a Spark StructType\n",
    "        \"\"\"\n",
    "        fields = []\n",
    "        for field_name, field_type in model.__annotations__.items():\n",
    "            # Check if the field is Optional\n",
    "            is_optional = 'typing.Optional' in str(field_type)\n",
    "            \n",
    "            # Get the actual type (remove Optional wrapper)\n",
    "            if is_optional and hasattr(field_type, '__args__'):\n",
    "                field_type = field_type.__args__[0]\n",
    "            \n",
    "            # Convert type\n",
    "            spark_type = _get_field_type(field_type)\n",
    "            \n",
    "            # Skip None types\n",
    "            if spark_type is None:\n",
    "                continue\n",
    "            \n",
    "            fields.append(StructField(field_name, spark_type, nullable=is_optional))\n",
    "        \n",
    "        return StructType(fields)\n",
    "\n",
    "    return _convert_model_to_struct(model_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98211b6d-7ab0-488c-a5e5-fd22b7f53da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_patient_record_schema():\n",
    "    return convert_pydantic_to_spark_schema(PatientRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0a4d64-cc60-4f26-809b-3e9a0499c175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_json_to_parquet(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert JSON to Parquet using dynamically generated schema\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to input JSON file\n",
    "        output_path (str): Path to output Parquet file\n",
    "    \"\"\"\n",
    "    # Create Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PatientRecord JSON to Parquet\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Get the dynamically generated schema\n",
    "    patient_record_schema = get_patient_record_schema()\n",
    "    \n",
    "    # Read JSON with explicit schema\n",
    "    df = spark.read.schema(patient_record_schema) \\\n",
    "        .json(input_path)\n",
    "    \n",
    "    # Write to Parquet\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    # Optional: Show schema and first few rows for verification\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85341e14-f086-46aa-adc7-59826706418b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = convert_json_to_parquet(\"dbfs:/FileStore/parsed_notes_jsonl.gz\", \"dbfs:/FileStore/notes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c451c010-91fd-448e-b2b7-4b081a0f9297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Basic Analytics and Visualizations\n",
    "\n",
    "- Rank medications by frequency of prescription\n",
    "- Analyze medication patterns across different demographic groups (e.g., top 3 per age group)\n",
    "- Identify and plot co-morbidity information from the patient records (e.g., hypertension, obesity, prediabetes, etc.) provided in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07720352-c3ad-4470-9b76-9afb1a7a8494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rank medications by frequency of prescription\n",
    "\n",
    "def analyze_medication_frequency(df):\n",
    "    # Explode medications to create a row per medication\n",
    "    medication_freq = df.filter(col(\"medications\").isNotNull()) \\\n",
    "        .select(explode(col(\"medications\")).alias(\"medication\"))\n",
    "    \n",
    "    # Count and rank medications\n",
    "    medication_ranking = medication_freq \\\n",
    "        .groupBy(\"medication.code\", \"medication.description\") \\\n",
    "        .agg(count(\"*\").alias(\"prescription_count\")) \\\n",
    "        .orderBy(desc(\"prescription_count\"))\n",
    "    \n",
    "    # Show top 20 medications\n",
    "    print(\"Top 20 Prescribed Medications:\")\n",
    "    medication_ranking.show(20, truncate=False)\n",
    "\n",
    "results = analyze_medication_frequency(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b415d2a-8430-40bd-bf87-5fd3f4481607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Additional advanced analysis\n",
    "def advanced_medication_analysis(df):\n",
    "    # Medications by demographic\n",
    "    medication_by_demographics = df \\\n",
    "        .filter(col(\"medications\").isNotNull()) \\\n",
    "        .select(\n",
    "            explode(col(\"medications\")).alias(\"medication\"),\n",
    "            col(\"demographics.gender\"),\n",
    "            col(\"demographics.age\")\n",
    "        )\n",
    "    \n",
    "    # Medication frequency by gender\n",
    "    gender_medication_freq = medication_by_demographics \\\n",
    "        .groupBy(\"medication.description\", \"gender\") \\\n",
    "        .agg(count(\"*\").alias(\"prescription_count\")) \\\n",
    "        .orderBy(desc(\"prescription_count\"))\n",
    "    \n",
    "    print(\"\\nMedication Prescription by Gender:\")\n",
    "    gender_medication_freq.show(20, truncate=False)\n",
    "    \n",
    "    # Medication frequency by age range\n",
    "    def categorize_age_range(age):\n",
    "        return (\n",
    "            when((age >= 0) & (age <= 17), \"0-17\")\n",
    "            .when((age >= 18) & (age <= 50), \"18-50\")\n",
    "            .when(age >= 51, \"51+\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "    \n",
    "    age_medication_freq = medication_by_demographics \\\n",
    "        .withColumn(\"age_range\", categorize_age_range(col(\"age\"))) \\\n",
    "        .groupBy(\"medication.description\", \"age_range\") \\\n",
    "        .agg(count(\"*\").alias(\"prescription_count\")) \\\n",
    "        .orderBy(desc(\"prescription_count\"))\n",
    "    \n",
    "    print(\"\\nMedication Prescription by Age Range:\")\n",
    "    age_medication_freq.show(20, truncate=False)\n",
    "    \n",
    "    return {\n",
    "        \"gender_medication_freq\": gender_medication_freq.collect(),\n",
    "        \"age_medication_freq\": age_medication_freq.collect()\n",
    "    }\n",
    "\n",
    "additional_analysis = advanced_medication_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fa0cf0-636f-4f60-9d74-4dd03485f386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify and plot co-morbidity information from the patient records (e.g., hypertension, obesity, prediabetes, etc.) provided in the dataset.\n",
    "\n",
    "# Prepare conditions data\n",
    "conditions_df = df.filter(col(\"conditions\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa87aeed-1f03-4115-a0a1-b02e25150942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group and count co-morbidities\n",
    "comorbidity_counts = conditions_df.groupBy(\"conditions\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "# Collect results\n",
    "comorbidity_results = comorbidity_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b7d8b7-4c14-41e4-b523-4e5303e097a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode the conditions list to flatten the DataFrame\n",
    "flattened_df = comorbidity_counts.withColumn(\"condition\", explode(col(\"conditions\"))).select(\n",
    "    col(\"condition.code\").alias(\"code\"),\n",
    "    col(\"condition.description\").alias(\"description\"),\n",
    "    col(\"count\")\n",
    ")\n",
    "\n",
    "# Group by condition code and description, and sum the counts\n",
    "result_df = flattened_df.groupBy(\"code\", \"description\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"total_count\")\n",
    "\n",
    "# Sort the results in descending order of total_count\n",
    "sorted_df = result_df.orderBy(col(\"total_count\").desc())\n",
    "\n",
    "# Display the sorted results\n",
    "sorted_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb6b97b-413a-43fe-a45b-040dd008988c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COVID-19 Case Demographics: Case breakdown by age ranges ([0-5], [6-10], [11-17], [18-30], [31-50], [51-70], [71+])\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = convert_json_to_parquet(\"dbfs:/FileStore/parsed_notes_jsonl.gz\", \"dbfs:/FileStore/notes.parquet\")\n",
    "\n",
    "new_df = df.select(\"demographics.age\").withColumn(\"Age Ranges\", when(col(\"age\").between(0, 5), \"[0-5]\").when(col(\"age\").between(6, 10), \"[6-10]\").when(col(\"age\").between(11, 17), \"[11-17]\").when(col(\"age\").between(18, 30), \"[18-30]\").when(col(\"age\").between(31, 50), \"[31-50]\").when(col(\"age\").between(51, 70), \"[51-70]\").when(col(\"age\") > 70, \"[71+]\")).groupBy(\"Age Ranges\").count()\n",
    "\n",
    "final_df = new_df.withColumn(\"sort\", when(col(\"Age Ranges\") == \"[0-5]\", 1).when(col(\"Age Ranges\") == \"[6-10]\", 2).when(col(\"Age Ranges\") == \"[11-17]\", 3).when(col(\"Age Ranges\") == \"[18-30]\", 4).when(col(\"Age Ranges\") == \"[31-50]\", 5).when(col(\"Age Ranges\") == \"[51-70]\", 6).when(col(\"Age Ranges\") == \"[71+]\", 7)).orderBy(\"sort\").select(\"Age Ranges\", \"count\")\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e3980e-7ae0-4f6f-b2b4-4fe3172102af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cumulative case count of Covid between the earliest case observed in the dataset and last case observed\n",
    "\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "df = convert_json_to_parquet(\"dbfs:/FileStore/parsed_notes_jsonl.gz\", \"dbfs:/FileStore/notes.parquet\")\n",
    "\n",
    "new_df_2 = df.select(\"encounter.date\").filter(col(\"date\") !=\"null\").sort(\"date\")\n",
    "print(new_df_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0992ed-435c-4dc3-8e9c-ea9e88eca449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## This visualization helps us determine the Oxygen Saturation status for all COVID-19 patients. We create the Oxygen Saturation range based on the standard scale and then count the number of COVID-19 patients who got a value located at these levels and see whether there is a relationship between Oxygen Saturation and COVID-19 encounters. In other words, whether COVID-19 will affect a person's Oxygen Saturation.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = convert_json_to_parquet(\"dbfs:/FileStore/parsed_notes_jsonl.gz\", \"dbfs:/FileStore/notes.parquet\")\n",
    "\n",
    "df = df.select(\"vitals.current.oxygen_saturation.value\").withColumn(\"Oxygen Saturation Ranges\", when(col(\"value\").between(98, 100), \"Normal\").when(col(\"value\").between(95, 97), \"Insufficient\").when(col(\"value\").between(90, 94), \"Decreased\").when(col(\"value\").between(80, 89), \"Critical\").when(col(\"value\").between(70, 79), \"Severe hypoxia\").when(col(\"value\") < 70, \"Acute danger to life\")).groupBy(\"Oxygen Saturation Ranges\").count()\n",
    "\n",
    "df = df.filter(df[\"Oxygen Saturation Ranges\"].isNotNull())\n",
    "\n",
    "df = df.withColumn(\"sort\", when(col(\"Oxygen Saturation Ranges\") == \"Normal\", 1).when(col(\"Oxygen Saturation Ranges\") == \"Insufficient\", 2).when(col(\"Oxygen Saturation Ranges\") == \"Decreased\", 3).when(col(\"Oxygen Saturation Ranges\") == \"Critical\", 4).when(col(\"Oxygen Saturation Ranges\") == \"Severe hypoxia\", 5).when(col(\"Oxygen Saturation Ranges\") == \"Acute danger to life\", 6)).orderBy(\"sort\").select(\"Oxygen Saturation Ranges\", \"count\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d24982-b264-442c-8f1a-98fca008132d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## This visualization can help us determine the relationship between COVID-19 and Weight. We create the weight range based on the standard scale and then count the number of COVID-19 patients who weigh at those levels to see whether there is a relationship between weight and COVID-19 encounters.\n",
    "\n",
    "df = convert_json_to_parquet(\"dbfs:/FileStore/parsed_notes_jsonl.gz\", \"dbfs:/FileStore/notes.parquet\")\n",
    "\n",
    "df = df.select(\"vitals.current.weight.value\").withColumn(\"Common Weight Ranges (kg)\", when(col(\"value\").between(0, 2.5), \"0 - 2.5\").when(col(\"value\").between(2.6, 10), \"2.6 - 10\").when(col(\"value\").between(11, 20), \"11 - 20\").when(col(\"value\").between(21, 40), \"21 - 40\").when(col(\"value\").between(41, 60), \"41 - 60\").when(col(\"value\").between(61, 80), \"61 - 80\").when(col(\"value\").between(81, 100), \"81 - 100\").when(col(\"value\").between(101, 120), \"101 - 120\").when(col(\"value\") > 120, \"121+\")).groupBy(\"Common Weight Ranges (kg)\").count()\n",
    "\n",
    "df = df.filter(df[\"Common Weight Ranges (kg)\"].isNotNull())\n",
    "\n",
    "df = df.withColumn(\"sort\", when(col(\"Common Weight Ranges (kg)\") == \"0 - 2.5\", 1).when(col(\"Common Weight Ranges (kg)\") == \"2.6 - 10\", 2).when(col(\"Common Weight Ranges (kg)\") == \"11 - 20\", 3).when(col(\"Common Weight Ranges (kg)\") == \"21 - 40\", 4).when(col(\"Common Weight Ranges (kg)\") == \"41 - 60\", 5).when(col(\"Common Weight Ranges (kg)\") == \"61 - 80\", 6).when(col(\"Common Weight Ranges (kg)\") == \"81 - 100\", 7).when(col(\"Common Weight Ranges (kg)\") == \"101 - 120\", 8).when(col(\"Common Weight Ranges (kg)\") == \"121+\", 9)).orderBy(\"sort\").select(\"Common Weight Ranges (kg)\", \"count\")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Assignment_3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
